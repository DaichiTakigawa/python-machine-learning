{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:03:40\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Counting words occurrences')\n",
    "for i, review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' ' + c + ' '\n",
    "                    for c in review]).lower()\n",
    "    df.loc[i, 'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map reviews to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:07\n"
     ]
    }
   ],
   "source": [
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "sequence_length = 200\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x) // batch_size\n",
    "    x = x[:n_batches * batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches * batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii + batch_size], y[ii:ii + batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii + batch_size]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SentimentRNN:\n",
    "\n",
    "    def __init__(self, n_words, seq_len=200, lstm_size=256, num_layers=1,\n",
    "                 batch_size=64, learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.compat.v1.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.compat.v1.train.Saver()\n",
    "            self.init_op = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "    def build(self):\n",
    "        tf_x = tf.compat.v1.placeholder(tf.int32,\n",
    "                                        shape=(self.batch_size, self.seq_len),\n",
    "                                        name='tf_x')\n",
    "        tf_y = tf.compat.v1.placeholder(tf.float32,\n",
    "                                        shape=self.batch_size,\n",
    "                                        name='tf_y')\n",
    "        tf_keepprob = tf.compat.v1.placeholder(tf.float32,\n",
    "                                               name='tf_keepprob')\n",
    "\n",
    "        # 埋め込み層を作成\n",
    "        embedding = tf.Variable(tf.compat.v1.random_uniform((self.n_words, self.embed_size),\n",
    "                                                            minval=-1, maxval=1),\n",
    "                                name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x,\n",
    "                                         name='embeded_x')\n",
    "\n",
    "        cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell(\n",
    "            [tf.compat.v1.nn.rnn_cell.DropoutWrapper(\n",
    "                tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.lstm_size),\n",
    "                output_keep_prob=tf_keepprob)\n",
    "                for _ in range(self.num_layers)])\n",
    "\n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print('  << initial state >> ', self.initial_state)\n",
    "\n",
    "        lstm_outputs, self.final_state = tf.compat.v1.nn.dynamic_rnn(\n",
    "            cells, embed_x, initial_state=self.initial_state)\n",
    "\n",
    "        print('\\n << lstm_output >> ', lstm_outputs)\n",
    "        print('\\n << final state >> ', self.final_state)\n",
    "\n",
    "        logits = tf.compat.v1.layers.dense(inputs=lstm_outputs[:, -1],\n",
    "                                           units=1, activation=None,\n",
    "                                           name='logits')\n",
    "\n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print('\\n << logits      >> ', logits)\n",
    "\n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels': tf.cast(tf.round(y_proba), tf.int32, name='labels')\n",
    "        }\n",
    "\n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y,\n",
    "                                                    logits=logits),\n",
    "            name='cost')\n",
    "\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.compat.v1.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "\n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                        X_train, y_train, self.batch_size):\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': 0.5,\n",
    "                            self.initial_state: state}\n",
    "                    loss, _, state = sess.run(\n",
    "                        ['cost:0', 'train_op', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "\n",
    "                    if iteration % 20 == 0:\n",
    "                        print('Epoch: %d/%d Iteration: %d | Train loss: %.5f' %\n",
    "                              (epoch + 1, num_epochs, iteration, loss))\n",
    "                    iteration += 1\n",
    "\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    self.saver.save(sess, 'model/sentiment-%d.ckpt' % epoch)\n",
    "\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.compat.v1.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, tf.compat.v1.train.latest_checkpoint('./model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(create_batch_generator(\n",
    "                    X_data, None, batch_size=self.batch_size), 1):\n",
    "                feed = {'tf_x:0': batch_x, 'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: test_state}\n",
    "\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['labels:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "\n",
    "                preds.append(pred)\n",
    "\n",
    "        return np.concatenate(preds)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "  << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "WARNING:tensorflow:From <ipython-input-8-a9f71b087225>:49: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\users\\1023d\\.virtualenvs\\pythonmachinelearning-qwvwlql2\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:759: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\1023d\\.virtualenvs\\pythonmachinelearning-qwvwlql2\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:708: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
      "c:\\users\\1023d\\.virtualenvs\\pythonmachinelearning-qwvwlql2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " << lstm_output >>  Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      " << final state >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Identity_4:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Identity_5:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      " << logits      >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\1023d\\.virtualenvs\\pythonmachinelearning-qwvwlql2\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "c:\\users\\1023d\\.virtualenvs\\pythonmachinelearning-qwvwlql2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words,\n",
    "                   seq_len=sequence_length,\n",
    "                   embed_size=256,\n",
    "                   lstm_size=128,\n",
    "                   num_layers=1,\n",
    "                   batch_size=100,\n",
    "                   learning_rate=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 Iteration: 20 | Train loss: 0.69299\n",
      "Epoch: 1/20 Iteration: 40 | Train loss: 0.61028\n",
      "Epoch: 1/20 Iteration: 60 | Train loss: 0.65630\n",
      "Epoch: 1/20 Iteration: 80 | Train loss: 0.49794\n",
      "Epoch: 1/20 Iteration: 100 | Train loss: 0.49802\n",
      "Epoch: 1/20 Iteration: 120 | Train loss: 0.50132\n",
      "Epoch: 1/20 Iteration: 140 | Train loss: 0.53061\n",
      "Epoch: 1/20 Iteration: 160 | Train loss: 0.43533\n",
      "Epoch: 1/20 Iteration: 180 | Train loss: 0.52311\n",
      "Epoch: 1/20 Iteration: 200 | Train loss: 0.42699\n",
      "Epoch: 1/20 Iteration: 220 | Train loss: 0.39022\n",
      "Epoch: 1/20 Iteration: 240 | Train loss: 0.45892\n",
      "Epoch: 2/20 Iteration: 260 | Train loss: 0.33109\n",
      "Epoch: 2/20 Iteration: 280 | Train loss: 0.39500\n",
      "Epoch: 2/20 Iteration: 300 | Train loss: 0.43369\n",
      "Epoch: 2/20 Iteration: 320 | Train loss: 0.42166\n",
      "Epoch: 2/20 Iteration: 340 | Train loss: 0.40529\n",
      "Epoch: 2/20 Iteration: 360 | Train loss: 0.31282\n",
      "Epoch: 2/20 Iteration: 380 | Train loss: 0.28152\n",
      "Epoch: 2/20 Iteration: 400 | Train loss: 0.39239\n",
      "Epoch: 2/20 Iteration: 420 | Train loss: 0.29512\n",
      "Epoch: 2/20 Iteration: 440 | Train loss: 0.29878\n",
      "Epoch: 2/20 Iteration: 460 | Train loss: 0.40166\n",
      "Epoch: 2/20 Iteration: 480 | Train loss: 0.24264\n",
      "Epoch: 2/20 Iteration: 500 | Train loss: 0.19893\n",
      "Epoch: 3/20 Iteration: 520 | Train loss: 0.23871\n",
      "Epoch: 3/20 Iteration: 540 | Train loss: 0.27368\n",
      "Epoch: 3/20 Iteration: 560 | Train loss: 0.34072\n",
      "Epoch: 3/20 Iteration: 580 | Train loss: 0.19533\n",
      "Epoch: 3/20 Iteration: 600 | Train loss: 0.26901\n",
      "Epoch: 3/20 Iteration: 620 | Train loss: 0.31830\n",
      "Epoch: 3/20 Iteration: 640 | Train loss: 0.29678\n",
      "Epoch: 3/20 Iteration: 660 | Train loss: 0.17979\n",
      "Epoch: 3/20 Iteration: 680 | Train loss: 0.29037\n",
      "Epoch: 3/20 Iteration: 700 | Train loss: 0.24669\n",
      "Epoch: 3/20 Iteration: 720 | Train loss: 0.24074\n",
      "Epoch: 3/20 Iteration: 740 | Train loss: 0.27884\n",
      "Epoch: 4/20 Iteration: 760 | Train loss: 0.20828\n",
      "Epoch: 4/20 Iteration: 780 | Train loss: 0.18751\n",
      "Epoch: 4/20 Iteration: 800 | Train loss: 0.22153\n",
      "Epoch: 4/20 Iteration: 820 | Train loss: 0.26481\n",
      "Epoch: 4/20 Iteration: 840 | Train loss: 0.12239\n",
      "Epoch: 4/20 Iteration: 860 | Train loss: 0.12158\n",
      "Epoch: 4/20 Iteration: 880 | Train loss: 0.22349\n",
      "Epoch: 4/20 Iteration: 900 | Train loss: 0.29672\n",
      "Epoch: 4/20 Iteration: 920 | Train loss: 0.10622\n",
      "Epoch: 4/20 Iteration: 940 | Train loss: 0.19704\n",
      "Epoch: 4/20 Iteration: 960 | Train loss: 0.16099\n",
      "Epoch: 4/20 Iteration: 980 | Train loss: 0.09309\n",
      "Epoch: 4/20 Iteration: 1000 | Train loss: 0.21147\n",
      "Epoch: 5/20 Iteration: 1020 | Train loss: 0.09780\n",
      "Epoch: 5/20 Iteration: 1040 | Train loss: 0.08132\n",
      "Epoch: 5/20 Iteration: 1060 | Train loss: 0.20302\n",
      "Epoch: 5/20 Iteration: 1080 | Train loss: 0.10009\n",
      "Epoch: 5/20 Iteration: 1100 | Train loss: 0.14064\n",
      "Epoch: 5/20 Iteration: 1120 | Train loss: 0.25124\n",
      "Epoch: 5/20 Iteration: 1140 | Train loss: 0.21405\n",
      "Epoch: 5/20 Iteration: 1160 | Train loss: 0.12091\n",
      "Epoch: 5/20 Iteration: 1180 | Train loss: 0.17132\n",
      "Epoch: 5/20 Iteration: 1200 | Train loss: 0.09771\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('Test Acc.: %.3f' % (np.sum(preds == y_true) / len(y_true)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "proba = rnn.predict(X_test, return_proba=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('pg2265.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[15858:]\n",
    "chars = set(text)\n",
    "char2int = {ch: i for i, ch in enumerate(chars)}\n",
    "int2char = dict(enumerate(chars))\n",
    "text_ints = np.array([char2int[ch] for ch in text], dtype=np.int32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    tot_batch_length = batch_size * num_steps\n",
    "    num_batches = int(len(sequence) / tot_batch_length)\n",
    "\n",
    "    if num_batches * tot_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "\n",
    "    x = sequence[0: num_batches * tot_batch_length]\n",
    "    y = sequence[1: num_batches * tot_batch_length + 1]\n",
    "\n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, tot_batch_length = data_x.shape\n",
    "    num_batchs = int(tot_batch_length / num_steps)\n",
    "    for b in range(num_batchs):\n",
    "        yield (data_x[:, b * num_steps:(b + 1) * num_steps],\n",
    "               data_y[:, b * num_steps:(b + 1) * num_steps])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_top_char(probas, char_size, top_n=5):\n",
    "    p = np.squeeze(probas)\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    p = p / np.sum(p)\n",
    "    ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "    return ch_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "class CharRNN:\n",
    "\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=100,\n",
    "                 lstm_size=128, num_layers=1, learning_rate=0.001,\n",
    "                 keep_prob=0.5, grad_clip=5, sampling=False):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.compat.v1.set_random_seed(123)\n",
    "\n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.compat.v1.train.Saver()\n",
    "            self.init_op = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "    def build(self, sampling):\n",
    "        if sampling:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps\n",
    "\n",
    "        tf_x = tf.compat.v1.placeholder(tf.int32,\n",
    "                                        shape=[batch_size, num_steps],\n",
    "                                        name='tf_x')\n",
    "        tf_y = tf.compat.v1.placeholder(tf.int32,\n",
    "                                        shape=[batch_size, num_steps],\n",
    "                                        name='tf_y')\n",
    "        tf_keepprob = tf.compat.v1.placeholder(tf.float32,\n",
    "                                               name='tf_keepprob')\n",
    "\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "\n",
    "        cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell(\n",
    "            [tf.compat.v1.nn.rnn_cell.DropoutWrapper(\n",
    "                tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.lstm_size),\n",
    "                output_keep_prob=tf_keepprob) for _ in range(self.num_layers)])\n",
    "\n",
    "        self.initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "        lstm_outputs, self.final_state = tf.compat.v1.nn.dynamic_rnn(\n",
    "            cells, x_onehot, initial_state=self.initial_state)\n",
    "\n",
    "        print(' << lstm_outputs >> ', lstm_outputs)\n",
    "\n",
    "        seq_output_reshaped = tf.reshape(lstm_outputs,\n",
    "                                         shape=[-1, self.lstm_size],\n",
    "                                         name='seq_output_reshaped')\n",
    "\n",
    "        logits = tf.compat.v1.layers.dense(inputs=seq_output_reshaped,\n",
    "                                           units=self.num_classes,\n",
    "                                           activation=None,\n",
    "                                           name='logits')\n",
    "\n",
    "        proba = tf.nn.softmax(logits, name='probabilities')\n",
    "\n",
    "        y_reshaped = tf.reshape(y_onehot,\n",
    "                                shape=[-1, self.num_classes],\n",
    "                                name='y_reshaped')\n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped),\n",
    "            name='cost')\n",
    "\n",
    "        tvars = tf.compat.v1.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                          self.grad_clip)\n",
    "\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                             name='train_op')\n",
    "\n",
    "    def train(self, train_x, train_y, num_epochs, ckpt_dir='./model/'):\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "\n",
    "        with tf.compat.v1.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "\n",
    "            n_batches = int(train_x.shape[1] / self.num_steps)\n",
    "            iterations = n_batches * num_epochs\n",
    "            for epoch in range(num_epochs):\n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "\n",
    "                bgen = create_batch_generator(train_x, train_y, self.num_steps)\n",
    "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                    iteration = epoch * n_batches + b\n",
    "                    feed = {'tf_x:0': batch_x, 'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': self.keep_prob,\n",
    "                            self.initial_state: new_state}\n",
    "                    batch_cost, _, new_state = sess.run(\n",
    "                        ['cost:0', 'train_op', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "\n",
    "                    if iteration % 10 == 0:\n",
    "                        print('Epoch %d/%d Iteration %d| Training loss: %.4f' %\n",
    "                              (epoch + 1, num_epochs, iteration, batch_cost))\n",
    "\n",
    "                self.saver.save(sess,\n",
    "                                os.path.join(ckpt_dir, 'language_modeling.ckpt'))\n",
    "\n",
    "    def sample(self, output_length, ckpt_dir, starter_seq='The '):\n",
    "        observed_seq = [ch for ch in starter_seq]\n",
    "        with tf.compat.v1.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess,\n",
    "                               tf.train.latest_checkpoint(ckpt_dir))\n",
    "\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0, 0] = char2int[ch]\n",
    "                feed = {'tf_x:0': x, 'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                    ['probabilities:0', self.final_state],\n",
    "                    feed_dict=feed)\n",
    "\n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "\n",
    "            for i in range(output_length):\n",
    "                x[0, 0] = ch_id\n",
    "                feed = {'tf_x:0': x, 'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                    ['probabilities:0', self.final_state],\n",
    "                    feed_dict=feed)\n",
    "\n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "\n",
    "        return ''.join(observed_seq)\n",
    "\n",
    "    def summary(self):\n",
    "        with self.g.as_default():\n",
    "            file_writer = tf.compat.v1.summary.FileWriter(logdir='./logs', graph=self.g)\n",
    "            file_writer.flush()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100\n",
    "train_x, train_y = reshape_data(text_ints, batch_size, num_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\1023d\\.virtualenvs\\pythonmachinelearning-qwvwlql2\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:708: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
      "c:\\users\\1023d\\.virtualenvs\\pythonmachinelearning-qwvwlql2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "c:\\users\\1023d\\.virtualenvs\\pythonmachinelearning-qwvwlql2\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "c:\\users\\1023d\\.virtualenvs\\pythonmachinelearning-qwvwlql2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " << lstm_outputs >>  Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "rnn.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rnn.train(train_x, train_y, num_epochs=100, ckpt_dir='./model-100/')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "del rnn\n",
    "np.rondom.seed(123)\n",
    "rnn = CharRNN(len(chars), sampling=True)\n",
    "print(rnn.sample(ckpt_dir='./model-100/', output_length=500))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (64, 2500)\n",
      "train_y shape: (64, 2500)\n"
     ]
    }
   ],
   "source": [
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[63, 12, 26, ..., 12, 21, 41],\n       [29, 26, 30, ..., 30, 64, 22],\n       [26, 27, 30, ..., 49, 32, 12],\n       ...,\n       [30, 39, 21, ..., 23, 49, 32],\n       [39, 30, 24, ..., 21, 41, 26],\n       [17, 26, 15, ..., 39, 18, 18]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([21, 41, 53, 21, 41,  4, 22, 30, 21, 53])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ints[60:70]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "160000"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}